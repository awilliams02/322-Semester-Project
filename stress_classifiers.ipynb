{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stress Detection\n",
    "By Alexa Williams and Mia Procel\n",
    "\n",
    "CPSC 322, Fall 2024\n",
    "\n",
    "_________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTRODUCTION\n",
    "_____________________________\n",
    "### Stress Detection Classifiers\n",
    "\n",
    "The dataset used comes from a study that followed 100 people for 30 days and kept track of different aspects of their life that might contribute to stress. Their PSS levels were measured each day.\n",
    "\n",
    "This notebook holds:\n",
    "\n",
    "* Dummy Classifier\n",
    "* kNN Classifier (k = 10)\n",
    "* Naive Bayes Classifier\n",
    "* Decision Tree Classifier \n",
    "* Random Forest Classifier\n",
    "\n",
    "The classifiers are created using k-fold cross validation with (k = 10).\n",
    "\n",
    "#### Our classifiers predict the discretized PSS_Score. The Random Forest Classifier performed the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mysklearn.mypytable.MyPyTable at 0x7f8a20265160>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "from tabulate import tabulate\n",
    "import random\n",
    "\n",
    "import mysklearn.myutils\n",
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable \n",
    "\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyKNeighborsClassifier, MyDummyClassifier, MyNaiveBayesClassifier, MyDecisionTreeClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation\n",
    "\n",
    "stress_data = MyPyTable()\n",
    "stress_data.load_from_file(\"cleaned_data.csv\")\n",
    "\n",
    "undiscretized_data = MyPyTable()\n",
    "undiscretized_data.load_from_file(\"stress_detection.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________\n",
    "\n",
    "## DATA ANALYSIS\n",
    "__________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics of the Stress Detection Data Before Discretization\n",
    "* We used these summary statistics to decide how to discretize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics: \n",
      "attribute                  min        max        mid        avg     median\n",
      "-----------------  -----------  ---------  ---------  ---------  ---------\n",
      "participant_id      1           100        50.5       50.5       50.5\n",
      "day                 1            30        15.5       15.5       15.5\n",
      "PSS_score          10            39        24.5       24.701     25\n",
      "Openness            1.005         4.9974    3.0012     3.02066    3.05012\n",
      "Conscientiousness   1.00098       4.99914   3.00006    3.00788    3.02206\n",
      "Extraversion        1.00058       4.99764   2.99911    3.0021     2.98555\n",
      "Agreeableness       1.00221       4.99988   3.00104    3.04766    3.09178\n",
      "Neuroticism         1.00017       4.99641   2.99829    2.96359    2.94095\n",
      "sleep_time          5.00329       8.99995   7.00162    7.00214    6.97822\n",
      "wake_time           5.00193       8.99837   7.00015    6.99057    6.98226\n",
      "sleep_duration      6.00056       8.99906   7.49981    7.47795    7.46342\n",
      "PSQI_score          1             4         2.5        2.49033    2\n",
      "call_duration       0.00288614   59.9831   29.993     29.7171    29.4652\n",
      "num_calls           0            19         9.5        9.362      9\n",
      "num_sms             0            49        24.5       24.4723    24\n",
      "screen_on_time      1.00687      11.9979    6.50237    6.62478    6.70097\n",
      "skin_conductance    0.501595      4.9991    2.75035    2.76202    2.769\n",
      "accelerometer       0.100791      2.49995   1.30037    1.31704    1.32042\n",
      "mobility_radius     0.100041      1.49989   0.799966   0.803164   0.793928\n",
      "mobility_distance   0.501622      4.99993   2.75078    2.80168    2.80107\n"
     ]
    }
   ],
   "source": [
    "summary_statistics = undiscretized_data.compute_summary_statistics(undiscretized_data.column_names)\n",
    "print(\"Summary Statistics: \")\n",
    "summary_statistics.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Class Imbalances\n",
    "\n",
    "\n",
    "### Here we will randomly sample instances of the data to use for the classifier. This will eliminate the class imbalances that the dataset originally contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete 650 high\n",
    "# delete 275 low\n",
    "random.seed(0)\n",
    "rows_with_high = [row for row in stress_data.data if row[2] == \"high\"]\n",
    "rows_to_drop = random.sample(rows_with_high, k=650)\n",
    "stress_data.data = [row for row in stress_data.data if row not in rows_to_drop]\n",
    "rows_with_low = [row for row in stress_data.data if row[2] == \"low\"]\n",
    "rows_to_drop = random.sample(rows_with_low, k=275)\n",
    "stress_data.data = [row for row in stress_data.data if row not in rows_to_drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________\n",
    "\n",
    "## CLASSIFICATION RESULTS\n",
    "________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We develop the following classifiers in this section:\n",
    "\n",
    "* Dummy Classifier\n",
    "* kNN Classifier (k = 10)\n",
    "* Naive Bayes Classifier\n",
    "* Decision Tree Classifier \n",
    "* Random Forest Classifier\n",
    "\n",
    "The classifiers are created using k-fold cross validation with (k = 10).\n",
    "\n",
    "Each classifier's class is inspired by the corresponding SciKitLearn classifier.\n",
    "\n",
    "### We test classifier accuracy using the following metrics:\n",
    "* Accuracy\n",
    "* Error rate\n",
    "* Precision\n",
    "* Recall\n",
    "* F1 measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Testing Classifiers with 10-Fold Cross Validation\n",
      "________________________\n",
      "\n",
      "Naive Bayes Classifier: accuracy = 0.34, error rate = 0.66, precision = 0.33, recall = 0.45, F1 = 0.38,\n",
      "\n",
      "k Nearest Neighbors Classifier: accuracy = 0.33, error rate = 0.67, precision = 0.32, recall = 0.34, F1 = 0.33,\n",
      "\n",
      "Dummy Classifier: accuracy = 0.31, error rate = 0.69, precision = 0.31, recall = 0.37, F1 = 0.34,\n",
      "\n",
      "Decision Tree Classifier: accuracy = 0.33, error rate = 0.67, precision = 0.33, recall = 0.33, F1 = 0.33,\n",
      "\n",
      "Random Forest Classifier: accuracy = 0.35, error rate = 0.65, precision = 0.34, recall = 0.74, F1 = 0.47,\n"
     ]
    }
   ],
   "source": [
    "X = [\n",
    "    [\n",
    "        row[15],  # screen_on_time\n",
    "        row[14],  # num_sms\n",
    "        row[13],  # num_calls\n",
    "        row[8]    # sleep_time\n",
    "    ]\n",
    "    for row in stress_data.data\n",
    "]\n",
    "y = stress_data.get_column(\"PSS_score\")\n",
    "header = [\"screen_on_time\", \"num_sms\", \"num_calls\", \"sleep_time\"]\n",
    "header_map = [\"att0\", \"att1\", \"att2\", \"att3\"]\n",
    "\n",
    "knn_avg_acc, knn_error_rate, nb_avg_acc, nb_error_rate, knn_y_actual, knn_y_pred, nb_y_actual, nb_y_pred, knn_binary_ps, nb_binary_ps, knn_recall, nb_recall, knn_f1, nb_f1, dummy_avg_acc, dummy_error_rate, dummy_binary_ps, dummy_recall, dummy_f1, dummy_y_actual, dummy_y_pred = myutils.knn_nb_classifiers(X, y)\n",
    "tree_avg_acc, tree_error_rate, tree_binary_ps, tree_recall, tree_f1, tree_y_actual, tree_y_pred = myutils.tree_classifier(X, y, header_map)\n",
    "forest_avg_acc, forest_error_rate, forest_binary_ps, forest_recall, forest_f1, forest_y_actual, forest_y_pred = myutils.forest_classifier(X, y)\n",
    "\n",
    "print(f\" Testing Classifiers with 10-Fold Cross Validation\")\n",
    "print(\"________________________\")\n",
    "print()\n",
    "print(f\"Naive Bayes Classifier: accuracy = {nb_avg_acc:.2f}, error rate = {nb_error_rate:.2f}, precision = {nb_binary_ps:.2f}, recall = {nb_recall:.2f}, F1 = {nb_f1:.2f},\")\n",
    "print()\n",
    "print(f\"k Nearest Neighbors Classifier: accuracy = {knn_avg_acc:.2f}, error rate = {knn_error_rate:.2f}, precision = {knn_binary_ps:.2f}, recall = {knn_recall:.2f}, F1 = {knn_f1:.2f},\")\n",
    "print()\n",
    "print(f\"Dummy Classifier: accuracy = {dummy_avg_acc:.2f}, error rate = {dummy_error_rate:.2f}, precision = {dummy_binary_ps:.2f}, recall = {dummy_recall:.2f}, F1 = {dummy_f1:.2f},\")\n",
    "print()\n",
    "print(f\"Decision Tree Classifier: accuracy = {tree_avg_acc:.2f}, error rate = {tree_error_rate:.2f}, precision = {tree_binary_ps:.2f}, recall = {tree_recall:.2f}, F1 = {tree_f1:.2f},\")\n",
    "print()\n",
    "print(f\"Random Forest Classifier: accuracy = {forest_avg_acc:.2f}, error rate = {forest_error_rate:.2f}, precision = {forest_binary_ps:.2f}, recall = {forest_recall:.2f}, F1 = {forest_f1:.2f},\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices\n",
    "\n",
    "* The section below displays a confusion matrix for each classifier that we built to showcase the performance of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 4: Confusion Matrices\n",
      "============================================================\n",
      "\n",
      "kNN Classifier (10-fold Cross Validation Confusion Matrix)\n",
      "\n",
      "PSS_score\n",
      "            high    low    moderate\n",
      "--------  ------  -----  ----------\n",
      "high         237    214         241\n",
      "low          249    199         244\n",
      "moderate     250    186         255\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Dummy Classifier (10-fold Cross Validation Confusion Matrix)\n",
      "\n",
      "PSS_score\n",
      "            high    low    moderate\n",
      "--------  ------  -----  ----------\n",
      "high         258    288         146\n",
      "low          282    262         148\n",
      "moderate     290    281         120\n",
      "------------------------------------------------------------\n",
      "\n",
      "Naive Bayes Classifier (10-fold Cross Validation Confusion Matrix)\n",
      "\n",
      "PSS_score\n",
      "            high    low    moderate\n",
      "--------  ------  -----  ----------\n",
      "high         308     96         288\n",
      "low          329     98         265\n",
      "moderate     308     80         303\n",
      "------------------------------------------------------------\n",
      "Decision Tree Classifier (10-fold Cross Validation Confusion Matrix)\n",
      "\n",
      "PSS_score\n",
      "            high    low    moderate\n",
      "--------  ------  -----  ----------\n",
      "high         227    219         246\n",
      "low          239    199         254\n",
      "moderate     218    218         255\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "Random Forest Classifier (10-fold Cross Validation Confusion Matrix)\n",
      "\n",
      "PSS_score\n",
      "            high    low    moderate\n",
      "--------  ------  -----  ----------\n",
      "high         509     33         150\n",
      "low          497     44         151\n",
      "moderate     474     45         172\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"============================================================\")\n",
    "print(\"STEP 4: Confusion Matrices\")\n",
    "print(\"============================================================\")\n",
    "print()\n",
    "labels = sorted(set(knn_y_actual) | set(knn_y_pred))\n",
    "labels_strings = list(map(str, labels))\n",
    "kNN_matrix = myevaluation.confusion_matrix(knn_y_actual, knn_y_pred, labels)\n",
    "print(\"kNN Classifier (10-fold Cross Validation Confusion Matrix)\")\n",
    "print()\n",
    "print(\"PSS_score\")\n",
    "print(tabulate(kNN_matrix, headers = labels_strings, showindex = labels_strings))\n",
    "print()\n",
    "print(\"------------------------------------------------------------\")\n",
    "print()\n",
    "print(\"Dummy Classifier (10-fold Cross Validation Confusion Matrix)\")\n",
    "print()\n",
    "print(\"PSS_score\")\n",
    "dummy_matrix = myevaluation.confusion_matrix(dummy_y_actual, dummy_y_pred, labels)\n",
    "print(tabulate(dummy_matrix, headers = labels_strings, showindex = labels_strings))\n",
    "print(\"------------------------------------------------------------\")\n",
    "print()\n",
    "print(\"Naive Bayes Classifier (10-fold Cross Validation Confusion Matrix)\")\n",
    "print()\n",
    "print(\"PSS_score\")\n",
    "labels = sorted(set(nb_y_actual) | set(nb_y_pred))\n",
    "labels_strings = list(map(str, labels))\n",
    "nb_matrix = myevaluation.confusion_matrix(nb_y_actual, nb_y_pred, labels)\n",
    "print(tabulate(nb_matrix, headers = labels_strings, showindex = labels_strings))\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Decision Tree Classifier (10-fold Cross Validation Confusion Matrix)\")\n",
    "print()\n",
    "print(\"PSS_score\")\n",
    "labels = sorted(set(tree_y_actual) | set(tree_y_pred))\n",
    "labels_strings = list(map(str, labels))\n",
    "tree_matrix = myevaluation.confusion_matrix(tree_y_actual, tree_y_pred, labels)\n",
    "print(tabulate(tree_matrix, headers = labels_strings, showindex = labels_strings))\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Random Forest Classifier (10-fold Cross Validation Confusion Matrix)\")\n",
    "print()\n",
    "print(\"PSS_score\")\n",
    "labels = sorted(set(forest_y_actual) | set(forest_y_pred))\n",
    "labels_strings = list(map(str, labels))\n",
    "forest_matrix = myevaluation.confusion_matrix(forest_y_actual, forest_y_pred, labels)\n",
    "print(tabulate(forest_matrix, headers = labels_strings, showindex = labels_strings))\n",
    "print(\"------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Results:\n",
    "\n",
    "After comparing the performance metrics for each classifier, it is clear to see the our random forest classifier performed the best overall! The dummy classifer was the worst classifier that we developed. The naive bayes, kNN, and decision tree classifiers all performed about the same right in the middle of the range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________\n",
    "\n",
    "## CLASSIFICATION WEB APP\n",
    "\n",
    "_____________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________\n",
    "\n",
    "## CONCLUSION\n",
    "\n",
    "_____________________________\n",
    "\n",
    "Our dataset comes from a study that followed 100 people for 30 days and kept track of different aspects of their life that might contribute to stress. Their PSS levels were measured each day.\n",
    "\n",
    "First we had to discretize the data. For PSS_Score we used the following rules to discretize\n",
    "* 0-20 = Low | 20-27 = Moderate | 27-40 = High\n",
    "\n",
    "Then, the dataset had major class imbalances that we had to deal with. We randomly selected 650 rows of the \"high\" class to delete and 275 rows of the \"low\" class to delete to diminish any biases in our classifiers. \n",
    "\n",
    "We developed a Dummy Classifier, kNN Classifier (k = 10), Naive Bayes Classifier, Decision Tree Classifier, and Random Forest Classifier. The classifiers were evaluated using k-fold cross validation with (k = 10).\n",
    "\n",
    "Overall, we learned that this data is not very adequate for predicting the PSS_Score of a person based on attributes such as \"screen_on_time\", \"num_sms\", \"num_calls\", and \"sleep_time\". However the random forest classifier had the best performance of all the classifiers we developed. It had an accuracy of 0.35, error rate of 0.65, precision of 0.34, recall of 0.74, and F1 of 0.47.\n",
    "\n",
    "If we were to work to improve our classification results we would have spent more time on the random forest classifier. We would have made the ensemble by choosing a couple decision trees fit specifically for each individual class. This could potentially make the ensemble perform much better. Another way to imporve the classifier accuracy would be to add a pruning feature to our decision tree class. This would make each decision tree have a more desirable performance, and in tern it would improve the ensemble.\n",
    "\n",
    "This project helps show that stress in not always easy to predict based on certain specific aspects of life. It can depend on many different things from person to person, and we cannot assume that two people with very similiar circumstances will feel the same PSS_Score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________\n",
    "\n",
    "## ACKNOWLEDGMENTS\n",
    "\n",
    "______________________________\n",
    "\n",
    "Dr. Sprint\n",
    "\n",
    "Models based on scikit learn classifiers\n",
    "\n",
    "ChatGPT for help conceptualizing how to set up our MyRandomForestClassifier class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
